{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This notebook focuses on answering the following questions:\n",
    "\n",
    "- Why does the L2 norm not work for logisitc regression? Can you show how what it looks like during training?\n",
    "- What types of loss functions are used for logistic regression?\n",
    "- How does multiclass logistic regression work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The algorithm for logistic regression is very simple, and in fact is very similar to that of linear regression. The model is simply given by:\n",
    "$$\\hat{\\underline{y}} = \\sigma\\left(\\underline{\\underline{X}}\\underline{w}\\right)$$\n",
    "\n",
    "Where $\\underline{\\underline{X}}$ and $\\underline{w}$ are created such that the bias term is captured, and $\\sigma$ is the sigmoid function, defined as:\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Note that the sigmoid function applied to a vector means that it is applied pointwise.\n",
    "\n",
    "Note that logisitc regression is still a linear model, since the weights are still linear. This means that the optimal weights $\\underline{w}^*$ multiply the data linearly. Even though there is a sigmoid function involved, this does not change the interaction between the weights.\n",
    "\n",
    "This is in contrast to a neural network, for example.\n",
    "\n",
    "In a simple 2 layer neural network with sigmoid activation functions, you have the following expressions:\n",
    "$$\\underline{\\underline{a}}^{(1)} = \\underline{\\underline{X}}\\cdot\\underline{\\underline{W}}^{(1)}$$\n",
    "$$\\underline{\\underline{z}}^{(1)} = \\sigma\\left(\\underline{\\underline{a}}^{(1)}\\right)$$\n",
    "$$\\underline{{a}}^{(2)} = \\underline{\\underline{z}}^{(1)}\\cdot\\underline{W}^{(2)}$$\n",
    "$$\\hat{\\underline{y}} = \\underline{{z}}^{(2)} = \\sigma\\left(\\underline{{a}}^{(2)}\\right)$$\n",
    "\n",
    "This means that our final predictor is:\n",
    "\n",
    "$$\\hat{\\underline{y}} = \\sigma\\left(\n",
    "\\sigma\\left(\n",
    "\\underline{\\underline{X}}\\cdot\\underline{\\underline{W}}^{(1)}\n",
    "\\right)\\cdot\\underline{W}^{(2)}\n",
    "\\right)$$\n",
    "\n",
    "We can see very clearly that the interaction between the weights is no longer linear. If we now ignore the activation function on the first layer, we arrive at logistic regression again:\n",
    "$$\\hat{\\underline{y}} = \\sigma\\left(\n",
    "\\left(\n",
    "\\underline{\\underline{X}}\\cdot\\underline{\\underline{W}}^{(1)}\n",
    "\\right)\\cdot\\underline{W}^{(2)}\n",
    "\\right) = \\sigma\\left(\\underline{\\underline{X}}\\cdot \\underline{\\hat{w}}\\right)$$\n",
    "\n",
    "Where $\\underline{\\hat{w}} = \\underline{\\underline{W}}^{(1)}\\underline{{W}}^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Current notes and feedback: this is very hardcoded. Can you look at how Logisitc models are implemented in sklearn and learn from that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, loss):\n",
    "        self.loss = loss\n",
    "        \n",
    "    def model(self, X, w):\n",
    "        y_hat = np.dot(X, w)\n",
    "        return sigmoid(y_hat)\n",
    "        \n",
    "    def sigmoid(self, arr):\n",
    "        return 1/(1+e^-arr)\n",
    "    \n",
    "    def sigmoid_prime(self, arr):\n",
    "        return e^-arr / ((1+e^-arr) ** 2)\n",
    "    \n",
    "    def grad(self, X, w): # should be a function the gradient of the loss function itself, as well as the model\n",
    "        return sigmoid_prime(np.dot(X, w))\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # add code to enable multiclass behaviour!\n",
    "        # add optimization!\n",
    "        gradient = self.loss.grad() * self.grad()\n",
    "        # need to define what gradient goes here, should depend on the LOSS\n",
    "        w_star = gd(gradient, X, y, **kwargs)\n",
    "        self.w_star = w_star\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model(X, self.w_star)\n",
    "    \n",
    "def gd(gradient, X, y, start = None, learning_rate = 0.01, n_iter = 100, tolerance = 1e-06):\n",
    "    m, n = X.shape\n",
    "    if not start:\n",
    "        start = np.random.rand(n)\n",
    "    \n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * np.array(gradient(x, y, vector))\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "# not sure if this is a smart way of intialising loss function? Can you find a better implementation?\n",
    "# in this case __init__ is just useless no?\n",
    "class l2_loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, pred, act):\n",
    "        loss = 0\n",
    "        for pred_, act_ in zip(pred, act):\n",
    "            loss += (pred_ - act_)**2\n",
    "        return loss / len(pred)\n",
    "    \n",
    "    def grad(pred, act):\n",
    "        \n",
    "        # I am stuck, not sure how to proceed.............\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "def l2_loss(pred, act):\n",
    "    loss = 0\n",
    "    for pred_, act_ in zip(pred, act):\n",
    "        loss += (pred_ - act_)**2\n",
    "    return loss / len(pred)\n",
    "\n",
    "def logistic_loss(pred, act):\n",
    "    # requires actual data to be either 0 or 1!\n",
    "    # this is also known as binary cross entropy\n",
    "    loss = 0\n",
    "    for pred_, act_ in zip(pred, act):\n",
    "        loss += act*math.log(pred) + (1-act)*log(1-pred)\n",
    "    return loss / - len(pred)\n",
    "\n",
    "\n",
    "# ensure gd is compatible with other models and thus not just part of this class!\n",
    "def gd(self, X, y, model, grad, alpha=0.01, max_iter = 100, precision = 0.0001):\n",
    "    m, n = X.shape\n",
    "    w_prev = np.random.rand(n) # this creates a uniform distribution for the weights, add options for choosing the intialisation\n",
    "    iteration = 0\n",
    "    step_size = 1\n",
    "    while step_size > precision and iteration < max_iter:\n",
    "        # need to add conditions to this to ensure that the optimization stops\n",
    "        # why not base the optimisation on early stopping?\n",
    "        w_star = w_prev - alpha * grad(w_prev, X)\n",
    "        y_hat = model(X, w_star)\n",
    "        loss = loss_function(y_hat, y)\n",
    "\n",
    "\n",
    "\n",
    "        # update previous values\n",
    "        step_size = abs(w_star - w_prev)\n",
    "        w_prev = w_star\n",
    "        iteration += 1\n",
    "\n",
    "        # need to add things to prevent it from overshooting\n",
    "        # compare loss to before\n",
    "\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does the L2 norm not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations with different loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "\n",
    "https://realpython.com/gradient-descent-algorithm-python/\n",
    "https://stats.stackexchange.com/questions/93569/why-is-logistic-regression-a-linear-classifier\n",
    "https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1\n",
    "https://ml4a.github.io/ml4a/how_neural_networks_are_trained/\n",
    "https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn\n",
    "https://www.internalpointers.com/post/cost-function-logistic-regression\n",
    "https://stats.stackexchange.com/questions/174364/why-use-different-cost-function-for-linear-and-logistic-regression\n",
    "https://stats.stackexchange.com/questions/236028/how-to-solve-logistic-regression-using-ordinary-least-squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Imperial",
   "language": "python",
   "name": "ml_imperial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
