{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This notebook focuses on answering the following questions:\n",
    "\n",
    "- Why does the L2 norm not work for logisitc regression? Can you show how what it looks like during training?\n",
    "- What types of loss functions are used for logistic regression?\n",
    "- How does multiclass logistic regression work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The algorithm for logistic regression is very simple, and in fact is very similar to that of linear regression. The model is simply given by:\n",
    "$$\\hat{\\underline{y}} = \\sigma\\left(\\underline{\\underline{X}}\\underline{w}\\right)$$\n",
    "\n",
    "Where $\\underline{\\underline{X}}$ and $\\underline{w}$ are created such that the bias term is captured, and $\\sigma$ is the sigmoid function, defined as:\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Note that the sigmoid function applied to a vector means that it is applied pointwise.\n",
    "\n",
    "Note that logisitc regression is still a linear model, since the weights are still linear. This means that the optimal weights $\\underline{w}^*$ multiply the data linearly. Even though there is a sigmoid function involved, this does not change the interaction between the weights.\n",
    "\n",
    "This is in contrast to a neural network, for example.\n",
    "\n",
    "In a simple 2 layer neural network with sigmoid activation functions, you have the following expressions:\n",
    "$$\\underline{\\underline{a}}^{(1)} = \\underline{\\underline{X}}\\cdot\\underline{\\underline{W}}^{(1)}$$\n",
    "$$\\underline{\\underline{z}}^{(1)} = \\sigma\\left(\\underline{\\underline{a}}^{(1)}\\right)$$\n",
    "$$\\underline{{a}}^{(2)} = \\underline{\\underline{z}}^{(1)}\\cdot\\underline{W}^{(2)}$$\n",
    "$$\\hat{\\underline{y}} = \\underline{{z}}^{(2)} = \\sigma\\left(\\underline{{a}}^{(2)}\\right)$$\n",
    "\n",
    "This means that our final predictor is:\n",
    "\n",
    "$$\\hat{\\underline{y}} = \\sigma\\left(\n",
    "\\sigma\\left(\n",
    "\\underline{\\underline{X}}\\cdot\\underline{\\underline{W}}^{(1)}\n",
    "\\right)\\cdot\\underline{W}^{(2)}\n",
    "\\right)$$\n",
    "\n",
    "We can see very clearly that the interaction between the weights is no longer linear. If we now ignore the activation function on the first layer, we arrive at logistic regression again:\n",
    "$$\\hat{\\underline{y}} = \\sigma\\left(\n",
    "\\left(\n",
    "\\underline{\\underline{X}}\\cdot\\underline{\\underline{W}}^{(1)}\n",
    "\\right)\\cdot\\underline{W}^{(2)}\n",
    "\\right) = \\sigma\\left(\\underline{\\underline{X}}\\cdot \\underline{\\hat{w}}\\right)$$\n",
    "\n",
    "Where $\\underline{\\hat{w}} = \\underline{\\underline{W}}^{(1)}\\underline{{W}}^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, loss):\n",
    "        self.loss = loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # add code to enable multiclass behaviour!\n",
    "        # add optimization!\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def l2_loss(pred, act):\n",
    "    loss = 0\n",
    "    for pred_, act_ in zip(pred, act):\n",
    "        loss += (pred_ - act_)**2\n",
    "    return loss / len(pred)\n",
    "\n",
    "def logistic_loss(pred, act):\n",
    "    # requires actual data to be either 0 or 1!\n",
    "    # this is also known as binary cross entropy\n",
    "    loss = 0\n",
    "    for pred_, act_ in zip(pred, act):\n",
    "        loss += act*math.log(pred) + (1-act)*log(1-pred)\n",
    "    return loss / - len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does the L2 norm not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations with different loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Imperial",
   "language": "python",
   "name": "ml_imperial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
