{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction to xgboost\n",
    "\n",
    "xgboost, generally speaking, is a [library](https://xgboost.readthedocs.io/en/latest/) designed to provide a ['gradient boosting'](https://en.wikipedia.org/wiki/Gradient_boosting) framework for different languages, including Python and Julia.\n",
    "\n",
    "It can run on single machines, or even distributed computing platforms such as Hadoop, Spark and Flint (all by Apache).\n",
    "\n",
    "It has recently risen to fame because it has enabled many winners in Kaggle competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gradient Boosting?\n",
    "\n",
    "When you use [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning), what you are doing is combining many different 'weak' learners to generalise a problem, creating a 'strong' learner.\n",
    "\n",
    "In general, the concept of boosting is to fit each 'tree' (here, a weak learner) to a [modified version of the dataset](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab). The data points that are modified are typiclly those which were difficult to classify in the first place. This modification allows the second tree classify the difficult data points.\n",
    "\n",
    "At first point the model is 'Tree 1 + Tree 2'. This process is repeated to a **specified number of iterations**. For each iteration, the new model tries to capture a new element that all the previous trees failed to capture.\n",
    "\n",
    "The final model is a weighted sum of the trees.\n",
    "\n",
    "The key parameter differentiating between different types of gradient boosters, such as AdaBoost and Gradient Boosting, is the manner in which the data is modified. \n",
    "\n",
    "As an example, AdaBoost changes the weights (makes them high) of the classification points to 'modify' the data. Gradient Boosters modify data based on a loss function. The latter is very advantageous because it allows the user to specify custom loss functions based on the problem at hand. For example, if you're trying to determine credit defaults (i.e. failure to repay debt), then you want to be able to predict bad loans. When you draw the FN, TN, FP, TP table, you notice that you're trying to maxmimise accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
