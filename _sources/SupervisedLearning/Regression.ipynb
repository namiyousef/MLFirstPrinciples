{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321261cd",
   "metadata": {},
   "source": [
    "# Introduction to Regression\n",
    "\n",
    "In a regression problem, we estimate a continuous function $f$ to estimate the true value $t$ for a given datapoint $\\underline{x}$, using information from our observed data $\\mathcal{S}_m=(\\underline{\\underline{X}}, \\underline{y})\\in\\mathbb{R}^{m\\times n}\\times \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad436f5",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "ðŸŸ¢ **Assumption:** Linear Regression assumes that the true data can be represented by a linear equation with some noise $\\underline{\\varepsilon}$. This is formally written as:\n",
    "\n",
    "$$\n",
    "\\underline{y} = \\underline{\\underline{X}}\\cdot\\underline{w} + \\underline{\\varepsilon}\n",
    "$$\n",
    "\n",
    "$\\underline{\\underline{X}}$ is known as the **design matrix**, and $\\underline{w}\\in{\\mathbb{R}^n}$ is a set of unknown weights that characterise the true distribution. The true target variable that we don't observe is given by $t = \\underline{x}^\\top \\cdot \\underline{w}$. In statistical texts, the weights are called parameters and are instead represented with $\\underline{\\beta}$.\n",
    "\n",
    "ðŸŸ¢ **Assumption:** For core linear regression problems, we assume that the errors are **i.i.d** with probability distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that the observed target variables are normally distributed: $\\underline{y}\\sim \\mathcal{N}(\\underline{\\underline{X}}\\cdot \\underline{w}, \\sigma^2\\underline{\\underline{\\delta}})$ [[P1]](#P1)\n",
    "\n",
    "## Inference\n",
    "\n",
    "In order to determine the optimal weights, we typically use the L2 norm. The total error from the observed data is the Sum of Squares (SS).\n",
    "\n",
    "$$\n",
    "S(\\underline{w}) = \\underline{\\varepsilon}^\\top\\cdot \\underline{\\varepsilon}\n",
    "$$\n",
    "\n",
    "The optimial weights that minimise the above expression are given by the expression below, called the **normal equations** [[P2]](#P2):\n",
    "\n",
    "$$\n",
    "\\underline{\\underline{X}}^\\top \\underline{\\underline{X}} \\cdot \\underline{\\hat{w}} = \\underline{\\underline{X}}^\\top \\underline{y}\n",
    "$$\n",
    "\n",
    "ðŸŸ¢ **Assumptions:** if we assume the following:\n",
    "- The number of observed points is greater than the number of predictor variables, $m>n$\n",
    "- $\\underline{\\underline{X}}$ is a full-rank matrix [[N1]](#N1)\n",
    "- Then our matrix is invertible [[N2]](#N2)\n",
    "\n",
    "We write [[N3]](#N3):\n",
    "\n",
    "$$\\underline{\\hat{w}} = (\\underline{\\underline{X}}^\\top \\underline{\\underline{X}})^{-1}\\underline{\\underline{X}}^\\top \\underline{y}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39885e69",
   "metadata": {},
   "source": [
    "# Other types of Regression\n",
    "\n",
    "## Regression and the two-sample test\n",
    "\n",
    "## Weighted Regression\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d192b",
   "metadata": {},
   "source": [
    "## Probabilistic View of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880fcde",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## Proofs\n",
    "<a id=\"P1\" href=\"../Appendix/A.Proofs/Regression.html#TargetVarDist\">[P1]</a>\n",
    "Derivation of the distribution of $\\underline{y}$\n",
    "\n",
    "<a id=\"P2\" href=\"../Appendix/A.Proofs/Regression.html#OLS\">[P2]</a>\n",
    "Ordinary Least Squares solution to Linear Regression\n",
    "\n",
    "## Notes\n",
    "\n",
    "<a id=\"N1\">[N1]</a>\n",
    "A full rank matrix $\\underline{\\underline{X}}\\in\\mathbb{R}^{m\\times n}$ is one where all $n$ columns are linearly independent. Thus if your design matrix has multicollinearity, then you cannot arrive at a unique solution to OLS. Even if you don't have perfect multicollinearity, terms that correlate highly with one another can still affect your OLS solutions due to computational errors when solving for $\\underline{w}$.\n",
    "\n",
    "<a id=\"N2\">[N2]</a>\n",
    "An invertible matrix must have all positive eigenvalues. This is a useful way to check for invertibility.\n",
    "\n",
    "<a id=\"N3\">[N3]</a>\n",
    "Typically we don't calculate the optimal weights using the inverse because it is computationally expensive. Instead we solve for a linear system of equations.\n",
    "\n",
    "## Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1d686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFirstPrinciples_dev",
   "language": "python",
   "name": "mlfirstprinciples_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
